{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4d4b318-f46e-4d2c-92cd-632027a03632",
   "metadata": {},
   "source": [
    "# Train RNN\n",
    "\n",
    "Here we give a complete example of what using Equinox normally looks like day-to-day.\n",
    "\n",
    "In this example we'll train an RNN to classify clockwise vs anticlockwise spirals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "690edba4-62d8-48ff-8ed2-2dd322399525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import jax\n",
    "import jax.lax as lax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import optax  # https://github.com/deepmind/optax\n",
    "\n",
    "import equinox as eqx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5423ff-c18b-4394-9d1d-0bb4bfd626d6",
   "metadata": {},
   "source": [
    "We begin by importing the usual libraries, setting up a very simple dataloader, and generating a toy dataset of spirals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14f40756-55d6-44f4-b13b-b15db35e15e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jrandom.permutation(key, indices)\n",
    "        (key,) = jrandom.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size\n",
    "\n",
    "\n",
    "def get_data(dataset_size, *, key):\n",
    "    t = jnp.linspace(0, 2 * math.pi, 16)\n",
    "    offset = jrandom.uniform(key, (dataset_size, 1), minval=0, maxval=2 * math.pi)\n",
    "    x1 = jnp.sin(t + offset) / (1 + t)\n",
    "    x2 = jnp.cos(t + offset) / (1 + t)\n",
    "    y = jnp.ones((dataset_size, 1))\n",
    "\n",
    "    half_dataset_size = dataset_size // 2\n",
    "    x1 = x1.at[:half_dataset_size].multiply(-1)\n",
    "    y = y.at[:half_dataset_size].set(0)\n",
    "    x = jnp.stack([x1, x2], axis=-1)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4094e1f-b1b5-4e3b-9191-bf09a5f93dda",
   "metadata": {},
   "source": [
    "Now for our model.\n",
    "\n",
    "Purely by way of example, we handle the final adding on of bias ourselves, rather than letting the `linear` layer do it. This is just so we can demonstrate how to use custom parameters in models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "533bdd6b-d660-4d94-bca9-da80ad0c0a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(eqx.Module):\n",
    "    hidden_size: int\n",
    "    cell: eqx.Module\n",
    "    linear: eqx.nn.Linear\n",
    "    bias: jnp.ndarray\n",
    "\n",
    "    def __init__(self, in_size, out_size, hidden_size, *, key):\n",
    "        ckey, lkey = jrandom.split(key)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = eqx.nn.GRUCell(in_size, hidden_size, key=ckey)\n",
    "        self.linear = eqx.nn.Linear(hidden_size, out_size, use_bias=False, key=lkey)\n",
    "        self.bias = jnp.zeros(out_size)\n",
    "\n",
    "    def __call__(self, input):\n",
    "        hidden = jnp.zeros((self.hidden_size,))\n",
    "\n",
    "        def f(carry, inp):\n",
    "            return self.cell(inp, carry), None\n",
    "\n",
    "        out, _ = lax.scan(f, hidden, input)\n",
    "        # sigmoid because we're performing binary classification\n",
    "        return jax.nn.sigmoid(self.linear(out) + self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb040df3-58c0-4546-b64d-c25147173e4b",
   "metadata": {},
   "source": [
    "And finally the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edef054d-5fdb-491e-8a46-c4d80363d012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=10000,\n",
    "    batch_size=32,\n",
    "    learning_rate=3e-3,\n",
    "    steps=200,\n",
    "    hidden_size=16,\n",
    "    depth=1,\n",
    "    seed=5678,\n",
    "):\n",
    "    data_key, loader_key, model_key = jrandom.split(jrandom.PRNGKey(seed), 3)\n",
    "    xs, ys = get_data(dataset_size, key=data_key)\n",
    "    iter_data = dataloader((xs, ys), batch_size, key=loader_key)\n",
    "\n",
    "    model = RNN(in_size=2, out_size=1, hidden_size=hidden_size, key=model_key)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    @eqx.filter_value_and_grad\n",
    "    def make_step(model, x, y):\n",
    "        pred_y = jax.vmap(model)(x)\n",
    "        # Trains with respect to binary cross-entropy\n",
    "        return -jnp.mean(y * jnp.log(pred_y) + (1 - y) * jnp.log(1 - pred_y))\n",
    "\n",
    "    optim = optax.adam(learning_rate)\n",
    "    opt_state = optim.init(model)\n",
    "    for step, (x, y) in zip(range(steps), iter_data):\n",
    "        loss, grads = make_step(model, x, y)\n",
    "        loss = loss.item()\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        print(f\"{step=}, {loss=}\")\n",
    "\n",
    "    pred_ys = jax.vmap(model)(xs)\n",
    "    num_correct = jnp.sum((pred_ys > 0.5) == ys)\n",
    "    final_accuracy = (num_correct / dataset_size).item()\n",
    "    print(f\"{final_accuracy=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6498efb9-f3d5-4dfb-b3f8-68f2b42422ec",
   "metadata": {},
   "source": [
    "`eqx.filter_jit` will look at all the arguments passed to `make_step`, and automatically JIT-trace every array and JIT-static everything else. For example the `model` parameters, and the data `x` and `y` will be traced. (However `model.hidden_size` is an integer, so it will be static'd instead.) This is a convenient way to JIT when your arguments are PyTrees, and if you need finer control then `eqx.filter_jit` takes a `filter_spec` argument.\n",
    "\n",
    "`eqx.filter_value_and_grad` will calculate the gradient with respect to the first argument (`model`). It will automatically do so for just the floating-point arrays in the `model` PyTree (i.e. its parameters) but not for anything else (e.g. `model.hidden_size`). Once again if you need finer control then `eqx.filter_value_and_grad` takes a `filter_spec` argument.\n",
    "\n",
    "_(Note that the above code is written for clarity, not speed. If you want maximum performance then the operations inside `dataloader`, and the `optim.update`, `eqx.apply_updates` operations, should all be brought inside the single JIT operation.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c05289e-2bd9-4f69-8a57-62d2f2f8047c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0, loss=0.695469856262207\n",
      "step=1, loss=0.6940110921859741\n",
      "step=2, loss=0.6931639909744263\n",
      "step=3, loss=0.6961429119110107\n",
      "step=4, loss=0.6957006454467773\n",
      "step=5, loss=0.6920372247695923\n",
      "step=6, loss=0.6938458681106567\n",
      "step=7, loss=0.6933101415634155\n",
      "step=8, loss=0.6897573471069336\n",
      "step=9, loss=0.6940688490867615\n",
      "step=10, loss=0.6896819472312927\n",
      "step=11, loss=0.6991991996765137\n",
      "step=12, loss=0.691841185092926\n",
      "step=13, loss=0.6914103627204895\n",
      "step=14, loss=0.7029447555541992\n",
      "step=15, loss=0.7040634155273438\n",
      "step=16, loss=0.6862525939941406\n",
      "step=17, loss=0.700591504573822\n",
      "step=18, loss=0.6833503246307373\n",
      "step=19, loss=0.6996323466300964\n",
      "step=20, loss=0.6967240571975708\n",
      "step=21, loss=0.6946160793304443\n",
      "step=22, loss=0.6940494775772095\n",
      "step=23, loss=0.688583493232727\n",
      "step=24, loss=0.694453239440918\n",
      "step=25, loss=0.694945216178894\n",
      "step=26, loss=0.6929985284805298\n",
      "step=27, loss=0.6932469606399536\n",
      "step=28, loss=0.6898558735847473\n",
      "step=29, loss=0.6925642490386963\n",
      "step=30, loss=0.6929633617401123\n",
      "step=31, loss=0.6931982636451721\n",
      "step=32, loss=0.6919726133346558\n",
      "step=33, loss=0.6934778690338135\n",
      "step=34, loss=0.6924077272415161\n",
      "step=35, loss=0.6937430500984192\n",
      "step=36, loss=0.6933866739273071\n",
      "step=37, loss=0.6933223009109497\n",
      "step=38, loss=0.6918285489082336\n",
      "step=39, loss=0.6911637783050537\n",
      "step=40, loss=0.6929216384887695\n",
      "step=41, loss=0.695015549659729\n",
      "step=42, loss=0.6941317319869995\n",
      "step=43, loss=0.6889361143112183\n",
      "step=44, loss=0.6929722428321838\n",
      "step=45, loss=0.6959724426269531\n",
      "step=46, loss=0.6923878788948059\n",
      "step=47, loss=0.6954452395439148\n",
      "step=48, loss=0.6928846836090088\n",
      "step=49, loss=0.6919665932655334\n",
      "step=50, loss=0.6892142295837402\n",
      "step=51, loss=0.6910432577133179\n",
      "step=52, loss=0.689067006111145\n",
      "step=53, loss=0.7064827680587769\n",
      "step=54, loss=0.6837577223777771\n",
      "step=55, loss=0.6927756071090698\n",
      "step=56, loss=0.6882894039154053\n",
      "step=57, loss=0.6991082429885864\n",
      "step=58, loss=0.6848499774932861\n",
      "step=59, loss=0.6892214417457581\n",
      "step=60, loss=0.6934791207313538\n",
      "step=61, loss=0.6962975859642029\n",
      "step=62, loss=0.7012044191360474\n",
      "step=63, loss=0.6914117336273193\n",
      "step=64, loss=0.6984191536903381\n",
      "step=65, loss=0.6937364339828491\n",
      "step=66, loss=0.7050725817680359\n",
      "step=67, loss=0.6893508434295654\n",
      "step=68, loss=0.6935131549835205\n",
      "step=69, loss=0.693252444267273\n",
      "step=70, loss=0.693372368812561\n",
      "step=71, loss=0.6887596845626831\n",
      "step=72, loss=0.6937415599822998\n",
      "step=73, loss=0.6912466287612915\n",
      "step=74, loss=0.692828893661499\n",
      "step=75, loss=0.6928187012672424\n",
      "step=76, loss=0.6929498314857483\n",
      "step=77, loss=0.6911673545837402\n",
      "step=78, loss=0.6932731866836548\n",
      "step=79, loss=0.6969736218452454\n",
      "step=80, loss=0.692180871963501\n",
      "step=81, loss=0.6920971870422363\n",
      "step=82, loss=0.6940006613731384\n",
      "step=83, loss=0.6947032809257507\n",
      "step=84, loss=0.6884186863899231\n",
      "step=85, loss=0.6916593313217163\n",
      "step=86, loss=0.6919583678245544\n",
      "step=87, loss=0.6955068111419678\n",
      "step=88, loss=0.6922551989555359\n",
      "step=89, loss=0.7009901404380798\n",
      "step=90, loss=0.6891013383865356\n",
      "step=91, loss=0.6935603618621826\n",
      "step=92, loss=0.69370436668396\n",
      "step=93, loss=0.685250997543335\n",
      "step=94, loss=0.6848466396331787\n",
      "step=95, loss=0.6973309516906738\n",
      "step=96, loss=0.702495813369751\n",
      "step=97, loss=0.6917577981948853\n",
      "step=98, loss=0.6930680871009827\n",
      "step=99, loss=0.6879289150238037\n",
      "step=100, loss=0.6978393197059631\n",
      "step=101, loss=0.6950803399085999\n",
      "step=102, loss=0.6934637427330017\n",
      "step=103, loss=0.6898033618927002\n",
      "step=104, loss=0.6917957067489624\n",
      "step=105, loss=0.6918165683746338\n",
      "step=106, loss=0.6925389766693115\n",
      "step=107, loss=0.6907787322998047\n",
      "step=108, loss=0.6930015087127686\n",
      "step=109, loss=0.6905994415283203\n",
      "step=110, loss=0.6903047561645508\n",
      "step=111, loss=0.6895794868469238\n",
      "step=112, loss=0.6919896602630615\n",
      "step=113, loss=0.6929000616073608\n",
      "step=114, loss=0.6861156225204468\n",
      "step=115, loss=0.6916525363922119\n",
      "step=116, loss=0.6891536712646484\n",
      "step=117, loss=0.6949775218963623\n",
      "step=118, loss=0.6913626790046692\n",
      "step=119, loss=0.6878023147583008\n",
      "step=120, loss=0.6889688968658447\n",
      "step=121, loss=0.6912539601325989\n",
      "step=122, loss=0.688143253326416\n",
      "step=123, loss=0.6874911785125732\n",
      "step=124, loss=0.6872292757034302\n",
      "step=125, loss=0.6878771185874939\n",
      "step=126, loss=0.6867557764053345\n",
      "step=127, loss=0.6870120167732239\n",
      "step=128, loss=0.684597373008728\n",
      "step=129, loss=0.6766145825386047\n",
      "step=130, loss=0.6783543825149536\n",
      "step=131, loss=0.6876450777053833\n",
      "step=132, loss=0.6898150444030762\n",
      "step=133, loss=0.6808171272277832\n",
      "step=134, loss=0.6852248907089233\n",
      "step=135, loss=0.6800845265388489\n",
      "step=136, loss=0.6718021631240845\n",
      "step=137, loss=0.6681873798370361\n",
      "step=138, loss=0.6753413677215576\n",
      "step=139, loss=0.671585202217102\n",
      "step=140, loss=0.6637449264526367\n",
      "step=141, loss=0.6678113341331482\n",
      "step=142, loss=0.6511460542678833\n",
      "step=143, loss=0.650525689125061\n",
      "step=144, loss=0.6523746252059937\n",
      "step=145, loss=0.6346812844276428\n",
      "step=146, loss=0.6078983545303345\n",
      "step=147, loss=0.6069947481155396\n",
      "step=148, loss=0.6328916549682617\n",
      "step=149, loss=0.5921869277954102\n",
      "step=150, loss=0.5772275328636169\n",
      "step=151, loss=0.5446708798408508\n",
      "step=152, loss=0.545225977897644\n",
      "step=153, loss=0.5175529718399048\n",
      "step=154, loss=0.49300363659858704\n",
      "step=155, loss=0.41911324858665466\n",
      "step=156, loss=0.390226811170578\n",
      "step=157, loss=0.36768990755081177\n",
      "step=158, loss=0.30429911613464355\n",
      "step=159, loss=0.2722632884979248\n",
      "step=160, loss=0.2577134966850281\n",
      "step=161, loss=0.1969938725233078\n",
      "step=162, loss=0.13784314692020416\n",
      "step=163, loss=0.1397821009159088\n",
      "step=164, loss=0.11436247825622559\n",
      "step=165, loss=0.08968592435121536\n",
      "step=166, loss=0.07026629149913788\n",
      "step=167, loss=0.06773999333381653\n",
      "step=168, loss=0.0786045491695404\n",
      "step=169, loss=0.04946307837963104\n",
      "step=170, loss=0.039137713611125946\n",
      "step=171, loss=0.03697769343852997\n",
      "step=172, loss=0.03442113846540451\n",
      "step=173, loss=0.02817709371447563\n",
      "step=174, loss=0.02260049805045128\n",
      "step=175, loss=0.0208757221698761\n",
      "step=176, loss=0.0238012857735157\n",
      "step=177, loss=0.019132442772388458\n",
      "step=178, loss=0.016685867682099342\n",
      "step=179, loss=0.015729248523712158\n",
      "step=180, loss=0.01557326503098011\n",
      "step=181, loss=0.014118168503046036\n",
      "step=182, loss=0.012221714481711388\n",
      "step=183, loss=0.012281971052289009\n",
      "step=184, loss=0.011437132023274899\n",
      "step=185, loss=0.010124631226062775\n",
      "step=186, loss=0.010237202048301697\n",
      "step=187, loss=0.009541159495711327\n",
      "step=188, loss=0.009026993066072464\n",
      "step=189, loss=0.009005479514598846\n",
      "step=190, loss=0.009186260402202606\n",
      "step=191, loss=0.008103055879473686\n",
      "step=192, loss=0.007458947598934174\n",
      "step=193, loss=0.0075416406616568565\n",
      "step=194, loss=0.007497536949813366\n",
      "step=195, loss=0.006723443046212196\n",
      "step=196, loss=0.00683201476931572\n",
      "step=197, loss=0.006709215696901083\n",
      "step=198, loss=0.006567368749529123\n",
      "step=199, loss=0.006062147207558155\n",
      "final_accuracy=1.0\n"
     ]
    }
   ],
   "source": [
    "main()  # All right, let's run the code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax0226",
   "language": "python",
   "name": "jax0226"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
